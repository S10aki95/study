{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('python_38_ver_test': conda)"
  },
  "interpreter": {
   "hash": "0c94df9a04beabacc8fb55156216a9ca69dd7034d3e5b99b27b0417a6e766b4e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "df = pd.read_csv('livedoor_news.csv')\r\n",
    "label = df['label'].tolist()\r\n",
    "train = df['sentence'].tolist()\r\n",
    "\r\n",
    "df.head() #{0 : 'IT', 1 : 'ã‚¹ãƒãƒ¼ãƒ„', 2 : 'æ˜ ç”»'}"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0  label                                         sentence\n",
       "0        2068      2           ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ï¼šãƒãƒƒã‚¯Gç›£ç£ã€Œãƒªãƒ¡ã‚¤ã‚¯ã—ãŸã„æ—¥æœ¬ã®ã‚¢ãƒ‹ãƒ¡ã¯ã€AKIRAã€ã‹ãªã€\n",
       "1         898      1       ã€Sports Watchã€‘ãªã§ã—ã“ã‚¸ãƒ£ãƒ‘ãƒ³å‹åˆ©ã€æ±ºå‹ï¼ã‚¢ãƒ¡ãƒªã‚«æˆ¦ã®æ”»ç•¥ãƒã‚¤ãƒ³ãƒˆã¯â€¦\n",
       "2        1971      2          å¥³å­é«˜ç”Ÿã‚·ãƒ³ã‚¬ãƒ¼ã®KyleeãŒã€Œç¾äººæ˜ å†™æŠ€å¸«ã¨ã®æ‹ã€ã‚’æã„ãŸæ˜ ç”»ã®ä¸»é¡Œæ­Œæ‹…å½“ã«\n",
       "3        1288      1                    éŠ€ãƒ¡ãƒ€ãƒ«ãƒ»ä¸‰å®…å®å®Ÿã€è©¦åˆå¾Œã€åŒã˜è¨€è‘‰ãŒ32å›é£›ã³å‡ºã—ãŸ!?\n",
       "4         322      0  ä¸ƒäººã®ä¾ã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸæ–°ç´ æï¼ã€€NECã®LaVie Zã§ä½¿ã‚ã‚ŒãŸæ–°ç´ æã®é–‹ç™ºç§˜è©±ã€ãƒ‡ã‚¸é€šã€‘"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2068</td>\n",
       "      <td>2</td>\n",
       "      <td>ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ï¼šãƒãƒƒã‚¯Gç›£ç£ã€Œãƒªãƒ¡ã‚¤ã‚¯ã—ãŸã„æ—¥æœ¬ã®ã‚¢ãƒ‹ãƒ¡ã¯ã€AKIRAã€ã‹ãªã€</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>898</td>\n",
       "      <td>1</td>\n",
       "      <td>ã€Sports Watchã€‘ãªã§ã—ã“ã‚¸ãƒ£ãƒ‘ãƒ³å‹åˆ©ã€æ±ºå‹ï¼ã‚¢ãƒ¡ãƒªã‚«æˆ¦ã®æ”»ç•¥ãƒã‚¤ãƒ³ãƒˆã¯â€¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1971</td>\n",
       "      <td>2</td>\n",
       "      <td>å¥³å­é«˜ç”Ÿã‚·ãƒ³ã‚¬ãƒ¼ã®KyleeãŒã€Œç¾äººæ˜ å†™æŠ€å¸«ã¨ã®æ‹ã€ã‚’æã„ãŸæ˜ ç”»ã®ä¸»é¡Œæ­Œæ‹…å½“ã«</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1288</td>\n",
       "      <td>1</td>\n",
       "      <td>éŠ€ãƒ¡ãƒ€ãƒ«ãƒ»ä¸‰å®…å®å®Ÿã€è©¦åˆå¾Œã€åŒã˜è¨€è‘‰ãŒ32å›é£›ã³å‡ºã—ãŸ!?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>322</td>\n",
       "      <td>0</td>\n",
       "      <td>ä¸ƒäººã®ä¾ã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸæ–°ç´ æï¼ã€€NECã®LaVie Zã§ä½¿ã‚ã‚ŒãŸæ–°ç´ æã®é–‹ç™ºç§˜è©±ã€ãƒ‡ã‚¸é€šã€‘</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "train_text, test_text, train_label, test_label = train_test_split(train, label, train_size=0.8)\r\n",
    "\r\n",
    "JP_tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\r\n",
    "\r\n",
    "#ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\r\n",
    "train_encodings = JP_tokenizer(train_text, truncation= True, padding=True)\r\n",
    "test_encodings = JP_tokenizer(test_text, truncation= True, padding= True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\r\n",
    "    def __init__(self, encodings, labels):\r\n",
    "        self.encodings = encodings\r\n",
    "        self.labels = labels\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\r\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\r\n",
    "        return item\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.labels)\r\n",
    "\r\n",
    "train_dataset = IMDbDataset(train_encodings, train_label)\r\n",
    "val_dataset = IMDbDataset(test_encodings, test_label)"
   ],
   "outputs": [],
   "metadata": {
    "id": "3NifWyoLxmit"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "model = BertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', num_labels=3)\r\n",
    "\r\n",
    "# Linear(in_features=768, out_features=9, bias=True)\r\n",
    "\r\n",
    "training_args = TrainingArguments(\r\n",
    "    output_dir='./results',          # output directory\r\n",
    "    num_train_epochs=3,              # total number of training epochs\r\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\r\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\r\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\r\n",
    "    weight_decay=0.01,               # strength of weight decay\r\n",
    "    logging_dir='./logs',            # directory for storing logs\r\n",
    "    logging_steps=10,\r\n",
    ")\r\n",
    "\r\n",
    "trainer = Trainer(\r\n",
    "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\r\n",
    "    args=training_args,                  # training arguments, defined above\r\n",
    "    train_dataset=train_dataset,         # training dataset\r\n",
    "    eval_dataset=val_dataset             # evaluation dataset\r\n",
    ")\r\n",
    "\r\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 1/399 [00:07<48:51,  7.37s/it]"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-79b0f6851d15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\python_38_ver_test\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[0;32m   1051\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1053\u001b[1;33m                     \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1054\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python_38_ver_test\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1459\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1461\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python_38_ver_test\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python_38_ver_test\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}