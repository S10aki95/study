{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 分析の方針②について\r\n",
    "\r\n",
    "今回の目標：  \r\n",
    "基礎研究における分析方針②を実行する\r\n",
    "\r\n",
    "今回やること：  \r\n",
    "前処理によって得られたテキストデータと、株価のデータを用いて、Bertモデルのファインチューニングを行う。  \r\n",
    "①株価のデータ処理(株価の上昇率)  \r\n",
    "②特徴量の抽出  \r\n",
    "③LightGBMを最適化できるようにする  \r\n",
    "\r\n",
    "\r\n",
    "今回やらないこと：  \r\n",
    "テキストデータの前処理"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from transformers import BertTokenizer, BertModel, Trainer, TrainingArguments, AdamW\r\n",
    "from torch.optim import lr_scheduler\r\n",
    "from tqdm import tqdm\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#デイリーデータのインストール\r\n",
    "data = pd.read_csv('Daily_data.csv', index_col=0)\r\n",
    "data = data.groupby(level=0).tail(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Dataset(torch.utils.data.Dataset):\r\n",
    "    def __init__(self, encodings, labels):\r\n",
    "        self.encodings = encodings\r\n",
    "        self.labels = labels\r\n",
    "    \r\n",
    "    def __getitem__(self, idx):\r\n",
    "        item = {key : torch.tensor(val[idx]) for key, val in self.encodings.items()}\r\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\r\n",
    "        return item\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.labels)\r\n",
    "    \r\n",
    "    def get_labels(self):\r\n",
    "        return self.labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data['stock'] = data['stock'].pct_change()\r\n",
    "data.dropna(inplace=True)\r\n",
    "y = data['stock'].tolist()\r\n",
    "x = data['text'].tolist()\r\n",
    "date = list(data.index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test, date_train, date_test = train_test_split(x, y, date, shuffle = True)\r\n",
    "\r\n",
    "#トークン化するためのパッケージのインストール\r\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n",
    "\r\n",
    "#エンコーディング\r\n",
    "train_encodings = tokenizer(X_train, max_length = 128, truncation = True, padding = \"max_length\")\r\n",
    "test_encodings = tokenizer(X_train, max_length = 128, truncation = True, padding = \"max_length\")\r\n",
    "\r\n",
    "#データセット型に変換\r\n",
    "train_dataset = Dataset(train_encodings, y_train)\r\n",
    "val_dataset = Dataset(test_encodings, y_test)\r\n",
    "\r\n",
    "\r\n",
    "#データセット作成\r\n",
    "train_loader = DataLoader(train_dataset, batch_size=8)\r\n",
    "validation_loader = DataLoader(val_dataset, batch_size=8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class BertClassifier(nn.Module):\r\n",
    "    def __init__(self, linear_size = 1):\r\n",
    "        super(BertClassifier, self).__init__()\r\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\r\n",
    "\r\n",
    "        self.classifier = nn.Sequential(\r\n",
    "            nn.Dropout(p = 0.5),\r\n",
    "            nn.Linear(in_features=768, out_features=linear_size)\r\n",
    "            )\r\n",
    "    \r\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\r\n",
    "        bert_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\r\n",
    "        output = self.classifier(bert_output['last_hidden_state'][:,0,:])\r\n",
    "        return output\r\n",
    "    \r\n",
    "    def freeze_bert(self):\r\n",
    "        for param in self.bert.named_parameters():\r\n",
    "            param[1].requires_grad=False\r\n",
    "            \r\n",
    "    def unfreeze_bert(self):\r\n",
    "        for param in self.bert.named_parameters():\r\n",
    "            param[1].requires_grad=True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#model\r\n",
    "model = BertClassifier()\r\n",
    "\r\n",
    "#モデルをGPUに渡す(CPU)\r\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Decay_factor = 0.9\r\n",
    "learning_rate = 2.0e-5\r\n",
    "epochs = 3\r\n",
    "freeze_epochs = [4, 5]\r\n",
    "unfreeze_epochs = [1, 2, 3]\r\n",
    "\r\n",
    "#学習率の設定\r\n",
    "#Embedding Layer, Encoder Layer 0～11, Pooling Layer, Classifier Layerの、15個のレイヤーについての学習率を用意する\r\n",
    "LearingRate_list = [learning_rate * (Decay_factor ** i) for i in range(15)]\r\n",
    "Embedding_lr = [{'params' : model.bert.embeddings.parameters(), 'lr' : LearingRate_list[0], 'weight_decay' : 0.01}]\r\n",
    "Layers_lr = [{'params' : layer.parameters(), 'lr' : LearingRate_list[i+1], 'weight_decay' : 0.01} for i, layer in enumerate(model.bert.encoder.layer)]\r\n",
    "Pooling_lr = [{'params' : model.bert.pooler.parameters(), 'lr' : LearingRate_list[13], 'weight_decay' : 0.01}]\r\n",
    "Classifier_lr = [{'params' : model.classifier.parameters(), 'lr' : LearingRate_list[14], 'weight_decay' : 0.01}]\r\n",
    "\r\n",
    "#学習方法の指定\r\n",
    "optimizer = AdamW(Embedding_lr + Layers_lr + Pooling_lr + Classifier_lr, lr=learning_rate)#, eps=1e-8)\r\n",
    "#optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay = 0.1)\r\n",
    "\r\n",
    "#ラベル\"1\"についてのロスの重さを設定\r\n",
    "loss_func = nn.MSELoss()\r\n",
    "#scheduler = lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.1)\r\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: 0.1 ** epoch)\r\n",
    "history = []\r\n",
    "best_acc = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Accumulation_steps = 8\r\n",
    "total_loss = []\r\n",
    "running_loss =0\r\n",
    "\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    print(\"=== Epoch: \", epoch, \" / \", epochs, \" ===\")\r\n",
    "\r\n",
    "    #エポックによってはBERT自体の学習をするかどうかを判断する\r\n",
    "    if epoch in freeze_epochs:  # stop training bert\r\n",
    "        print(\"Freeze BERT\")\r\n",
    "        model.freeze_bert()\r\n",
    "        \r\n",
    "    if epoch in unfreeze_epochs:  # train bert\r\n",
    "        print(\"Unfreeze BERT\")\r\n",
    "        model.unfreeze_bert()\r\n",
    "\r\n",
    "    # ===== train =====\r\n",
    "    model.train()  # train mode\r\n",
    "    #print(\"バッチ学習の開始\", optimizer.param_groups[0][\"lr\"])\r\n",
    "\r\n",
    "    for i, batch in tqdm(enumerate(train_loader)): \r\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\r\n",
    "\r\n",
    "\r\n",
    "        input_ids = batch['input_ids']\r\n",
    "        token_type_ids = batch['token_type_ids']\r\n",
    "        attention_mask = batch['attention_mask']\r\n",
    "        y_batch = batch['labels']\r\n",
    "\r\n",
    "        #print(\"===============================================\",'\\n')\r\n",
    "        #print(\"学習時のラベルの比率\", sum(y_batch)/len(y_batch))\r\n",
    "\r\n",
    "        probas = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\r\n",
    "\r\n",
    "        batch_loss = loss_func(probas, y_batch.view(-1, 1))\r\n",
    "        running_loss += batch_loss.item()\r\n",
    "        batch_loss = batch_loss / Accumulation_steps #勾配累積ごとに平均化\r\n",
    "        batch_loss.backward()  # calculate gradient per (learnable) weight\r\n",
    "\r\n",
    "        if (i + 1) % Accumulation_steps == 0 or (i + 1) == len(train_loader):\r\n",
    "            optimizer.step()  # update (learnable) weights\r\n",
    "            if (i + 1) == len(train_loader):\r\n",
    "            #学習率の更新\r\n",
    "                scheduler.step()\r\n",
    "                print(\"学習率の確認：\", optimizer.param_groups[0][\"lr\"])\r\n",
    "\r\n",
    "            optimizer.zero_grad()  # reset the gradients\r\n",
    "\r\n",
    "        if (i + 1) % 100 == 0:\r\n",
    "            print('学習のロス', running_loss / 100)\r\n",
    "            total_loss.append(running_loss / 100)\r\n",
    "            running_loss = 0\r\n",
    "\r\n",
    "    print(\"-----------トレーニング終了-----------------------\")\r\n",
    "\r\n",
    "    # ===== validate =====\r\n",
    "    model.eval()  # evaluation mode\r\n",
    "    acc_val_total = 0\r\n",
    "    fbeta_val_total = 0\r\n",
    "\r\n",
    "    # metrics on val\r\n",
    "    accumulated_val_loss = 0 \r\n",
    "    for batch in validation_loader:\r\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\r\n",
    "\r\n",
    "        x_batch = batch['input_ids']\r\n",
    "        token_type_ids = batch['token_type_ids']\r\n",
    "        attention_mask = batch['attention_mask']\r\n",
    "        y_batch = batch['labels']\r\n",
    "\r\n",
    "        with torch.no_grad(): # no gradients, because no update is performed\r\n",
    "            probas = model(input_ids=x_batch, token_type_ids = token_type_ids, attention_mask=attention_mask)\r\n",
    "        loss = loss_func(probas, y_batch)\r\n",
    "        accumulated_val_loss += loss\r\n",
    "\r\n",
    "\r\n",
    "    avg_val_loss = accumulated_val_loss / len(validation_loader)\r\n",
    "\r\n",
    "    #print(\"Train Acc:\", acc_train_total)\r\n",
    "    print(\"Val Acc:\", acc_val_total, \"\\n\")\r\n",
    "    print(\"fbeta Acc:\", fbeta_val_total, \"\\n\")\r\n",
    "    if acc_val_total > best_acc:\r\n",
    "        best_acc = acc_val_total\r\n",
    "    \r\n",
    "    history.append({\"acc_val\": acc_val_total, \"fbeta_val\": fbeta_val_total})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== Epoch:  1  /  3  ===\n",
      "Unfreeze BERT\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-ddb491c2b5e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mAccumulation_steps\u001b[0m \u001b[1;31m#勾配累積ごとに平均化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# calculate gradient per (learnable) weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mAccumulation_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python_38_ver_test\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python_38_ver_test\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('python_38_ver_test': conda)"
  },
  "interpreter": {
   "hash": "0c94df9a04beabacc8fb55156216a9ca69dd7034d3e5b99b27b0417a6e766b4e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}